{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085739e3-dd6c-438a-b025-fa27e330cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-multipart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fffcf5-47c2-4bd6-a9e3-e8fd57bbbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Query, File, UploadFile\n",
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# --- Config ---\n",
    "DATA_PATH = '/mnt/data/cleaned_solar_data.csv'\n",
    "MODEL_DIR = '/tmp/solar_models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'solar_model.joblib')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('solar_backend')\n",
    "\n",
    "app = FastAPI(title='Solar Data Backend', version='1.1')\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# in-memory dataset\n",
    "_df: Optional[pd.DataFrame] = None\n",
    "_date_cols: List[str] = []\n",
    "\n",
    "# --- Utility functions ---\n",
    "\n",
    "def detect_and_parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Try to detect date-like columns and parse them to datetime.\"\"\"\n",
    "    global _date_cols\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
    "            candidates.append(col)\n",
    "            continue\n",
    "        # Heuristic: column name contains date/time keywords or dtype is object\n",
    "        if col.lower().count('date') or col.lower().count('time') or df[col].dtype == object:\n",
    "            try:\n",
    "                parsed = pd.to_datetime(df[col], errors='coerce', infer_datetime_format=True)\n",
    "                # if many non-nulls after parsing, accept\n",
    "                non_null_ratio = parsed.notna().mean()\n",
    "                if non_null_ratio > 0.6:\n",
    "                    df[col] = parsed\n",
    "                    candidates.append(col)\n",
    "            except Exception:\n",
    "                pass\n",
    "    _date_cols = candidates\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_df():\n",
    "    if _df is None:\n",
    "        raise HTTPException(status_code=500, detail=f'Data not loaded. Expected CSV at {DATA_PATH}')\n",
    "\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    load_data()   # call your existing loader\n",
    "    yield\n",
    "\n",
    "app = FastAPI(title='Solar Data Backend', version='1.1', lifespan=lifespan)\n",
    "\n",
    "def load_data():\n",
    "    global _df\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        try:\n",
    "            # read without forcing parse_dates; we'll detect\n",
    "            df = pd.read_csv(DATA_PATH)\n",
    "            df = detect_and_parse_dates(df)\n",
    "            _df = df\n",
    "            logger.info(f'Loaded dataframe with {len(_df)} rows and {len(_df.columns)} columns')\n",
    "        except Exception as e:\n",
    "            _df = None\n",
    "            logger.exception('Failed to load CSV')\n",
    "    else:\n",
    "        _df = None\n",
    "        logger.warning('Data file not found at %s', DATA_PATH)\n",
    "\n",
    "class TrainRequest(BaseModel):\n",
    "    feature_columns: List[str]\n",
    "    target_column: str\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    model_type: Optional[str] = 'linear'\n",
    "    scale: Optional[bool] = False\n",
    "\n",
    "    model_config = {\n",
    "        \"protected_namespaces\": ()\n",
    "    }\n",
    "\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    features: Dict[str, float]\n",
    "\n",
    "# --- Endpoints ---\n",
    "\n",
    "@app.get('/health')\n",
    "def health():\n",
    "    return {'status': 'ok', 'data_loaded': _df is not None}\n",
    "\n",
    "@app.get('/features')\n",
    "def features():\n",
    "    \"\"\"Return column names and dtypes.\"\"\"\n",
    "    ensure_df()\n",
    "    cols = [{ 'name': c, 'dtype': str(_df[c].dtype) } for c in _df.columns]\n",
    "    return {'n_rows': len(_df), 'columns': cols, 'date_columns': _date_cols}\n",
    "\n",
    "@app.get('/info')\n",
    "def info(sample: int = 3):\n",
    "    ensure_df()\n",
    "    sample_df = _df.head(sample).to_dict(orient='records')\n",
    "    return {'n_rows': len(_df), 'n_cols': len(_df.columns), 'sample': sample_df}\n",
    "\n",
    "@app.get('/data')\n",
    "def get_data(columns: Optional[str] = Query(None, description='Comma separated column names'),\n",
    "             offset: int = 0,\n",
    "             limit: int = 100):\n",
    "    ensure_df()\n",
    "    df = _df\n",
    "    if columns:\n",
    "        cols = [c.strip() for c in columns.split(',') if c.strip()]\n",
    "        missing = [c for c in cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise HTTPException(status_code=400, detail=f'Missing columns: {missing}')\n",
    "        df = df[cols]\n",
    "    total = len(df)\n",
    "    rows = df.iloc[offset:offset+limit].to_dict(orient='records')\n",
    "    return {'total': total, 'offset': offset, 'limit': limit, 'rows': rows}\n",
    "\n",
    "@app.get('/summary')\n",
    "def summary(columns: Optional[str] = Query(None, description='Comma separated column names')):\n",
    "    ensure_df()\n",
    "    df = _df\n",
    "    if columns:\n",
    "        cols = [c.strip() for c in columns.split(',') if c.strip()]\n",
    "        missing = [c for c in cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise HTTPException(status_code=400, detail=f'Missing columns: {missing}')\n",
    "        df = df[cols]\n",
    "    numeric = df.select_dtypes(include=[np.number])\n",
    "    if numeric.empty:\n",
    "        raise HTTPException(status_code=400, detail='No numeric columns to summarize')\n",
    "    desc = numeric.describe().to_dict()\n",
    "    return desc\n",
    "\n",
    "@app.get('/timeseries')\n",
    "def timeseries(date_col: str, value_col: str, freq: str = 'D', agg: str = 'mean',\n",
    "               start: Optional[str] = None, end: Optional[str] = None):\n",
    "    ensure_df()\n",
    "    df = _df.copy()\n",
    "    if date_col not in df.columns:\n",
    "        raise HTTPException(status_code=400, detail=f'date_col {date_col} not in columns')\n",
    "    if value_col not in df.columns:\n",
    "        raise HTTPException(status_code=400, detail=f'value_col {value_col} not in columns')\n",
    "\n",
    "    if not np.issubdtype(df[date_col].dtype, np.datetime64):\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce', infer_datetime_format=True)\n",
    "    df = df.dropna(subset=[date_col])\n",
    "    if start:\n",
    "        df = df[df[date_col] >= pd.to_datetime(start)]\n",
    "    if end:\n",
    "        df = df[df[date_col] <= pd.to_datetime(end)]\n",
    "    df = df.set_index(date_col)\n",
    "\n",
    "    allowed_aggs = {'mean': 'mean', 'sum': 'sum', 'median': 'median'}\n",
    "    if agg not in allowed_aggs:\n",
    "        raise HTTPException(status_code=400, detail=f'Unsupported agg {agg}. Use one of {list(allowed_aggs.keys())}')\n",
    "    try:\n",
    "        res = getattr(df[value_col].resample(freq), allowed_aggs[agg])()\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f'Resampling failed: {e}')\n",
    "    res = res.dropna()\n",
    "    out = [{'timestamp': idx.isoformat(), 'value': float(val)} for idx, val in res.items()]\n",
    "    return {'count': len(out), 'freq': freq, 'agg': agg, 'series': out}\n",
    "\n",
    "@app.get('/plot')\n",
    "def plot_col(col: str, kind: str = 'line', max_points: int = 1000):\n",
    "    ensure_df()\n",
    "    df = _df\n",
    "    if col not in df.columns:\n",
    "        raise HTTPException(status_code=400, detail=f'Column {col} not found')\n",
    "\n",
    "    # find x axis: prefer first detected date column\n",
    "    x = None\n",
    "    if _date_cols:\n",
    "        x = df[_date_cols[0]]\n",
    "    elif 'date' in df.columns:\n",
    "        x = pd.to_datetime(df['date'], errors='coerce')\n",
    "    elif np.issubdtype(df.index.dtype, np.datetime64):\n",
    "        x = df.index\n",
    "\n",
    "    y = df[col]\n",
    "    plot_df = pd.DataFrame({'y': y})\n",
    "    if x is not None:\n",
    "        plot_df['x'] = pd.to_datetime(x, errors='coerce')\n",
    "        plot_df = plot_df.dropna(subset=['y'])\n",
    "        # keep rows with x if x exists\n",
    "        plot_df = plot_df.sort_values('x')\n",
    "    else:\n",
    "        plot_df = plot_df.dropna(subset=['y']).reset_index(drop=True)\n",
    "\n",
    "    if len(plot_df) > max_points:\n",
    "        # sample evenly to preserve trend\n",
    "        idx = np.linspace(0, len(plot_df) - 1, max_points).astype(int)\n",
    "        plot_df = plot_df.iloc[idx]\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if kind == 'line':\n",
    "        if 'x' in plot_df.columns:\n",
    "            plt.plot(plot_df['x'], plot_df['y'])\n",
    "        else:\n",
    "            plt.plot(plot_df['y'].values)\n",
    "    elif kind == 'hist':\n",
    "        plt.hist(plot_df['y'].dropna().values, bins=50)\n",
    "    elif kind == 'box':\n",
    "        plt.boxplot(plot_df['y'].dropna().values)\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail='Unsupported kind')\n",
    "    plt.title(f'{kind} of {col}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    return StreamingResponse(buf, media_type='image/png')\n",
    "\n",
    "@app.post('/train')\n",
    "def train_model(req: TrainRequest):\n",
    "    ensure_df()\n",
    "    df = _df.copy()\n",
    "    # validate columns\n",
    "    missing = [c for c in req.feature_columns + [req.target_column] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise HTTPException(status_code=400, detail=f'Missing columns: {missing}')\n",
    "\n",
    "    X = df[req.feature_columns].select_dtypes(include=[np.number]).copy()\n",
    "    # drop rows with NA in features/target\n",
    "    data = X.join(df[[req.target_column]])\n",
    "    data = data.dropna()\n",
    "    if data.empty:\n",
    "        raise HTTPException(status_code=400, detail='No numeric training data after dropping NA')\n",
    "    X = data[req.feature_columns]\n",
    "    y = data[req.target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=req.test_size, random_state=req.random_state)\n",
    "\n",
    "    scaler = None\n",
    "    if req.scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    if req.model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif req.model_type == 'random_forest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=req.random_state)\n",
    "    else:\n",
    "        raise HTTPException(status_code=400, detail='Unsupported model_type; use \"linear\" or \"random_forest\"')\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = float(mean_squared_error(y_test, preds))\n",
    "\n",
    "    saved = {'model': model, 'features': req.feature_columns, 'model_type': req.model_type, 'scale': req.scale}\n",
    "    if scaler is not None:\n",
    "        saved['scaler'] = scaler\n",
    "    joblib.dump(saved, MODEL_PATH)\n",
    "    logger.info('Saved model to %s', MODEL_PATH)\n",
    "\n",
    "    return {'status': 'trained', 'mse': mse, 'n_train': len(X_train), 'n_test': len(X_test), 'model_path': MODEL_PATH}\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(req: PredictRequest):\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise HTTPException(status_code=400, detail='Model not trained. Call /train first.')\n",
    "    saved = joblib.load(MODEL_PATH)\n",
    "    model = saved.get('model')\n",
    "    features = saved.get('features')\n",
    "    scaler = saved.get('scaler') if saved.get('scale') else None\n",
    "\n",
    "    # Ensure all features are present\n",
    "    missing = [f for f in features if f not in req.features]\n",
    "    if missing:\n",
    "        raise HTTPException(status_code=400, detail=f'Missing features in request: {missing}')\n",
    "\n",
    "    # preserve order\n",
    "    x_arr = np.array([float(req.features[f]) for f in features]).reshape(1, -1)\n",
    "    if scaler is not None:\n",
    "        x_arr = scaler.transform(x_arr)\n",
    "    pred = model.predict(x_arr)\n",
    "    return {'prediction': float(pred[0]), 'features': features}\n",
    "\n",
    "@app.post('/upload_csv')\n",
    "async def upload_csv(file: UploadFile = File(...)):\n",
    "    if not file.filename.lower().endswith('.csv'):\n",
    "        raise HTTPException(status_code=400, detail='Only CSV supported')\n",
    "    content = await file.read()\n",
    "    with open(DATA_PATH, 'wb') as f:\n",
    "        f.write(content)\n",
    "    try:\n",
    "        load_data()\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f'Failed to reload data: {e}')\n",
    "    return {'status': 'uploaded', 'rows': len(_df) if _df is not None else 0}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import uvicorn\n",
    "    uvicorn.run('fastapi_solar_backend:app', host='0.0.0.0', port=8000, reload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648c6bf-7ccd-4486-bfc1-9c16e288fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86244588-72c7-4c61-a249-67cc068b982d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
